# ğŸ”’ AI Security & Privacy

Security vulnerabilities, privacy-preserving techniques, and defensive strategies for protecting AI systems from adversarial attacks and ensuring responsible data handling.

## ğŸ“– Overview

AI Security focuses on protecting machine learning models and systems from malicious attacks, including adversarial examples, model extraction, data poisoning, and privacy violations. This emerging field addresses critical challenges in deploying trustworthy AI systems in production environments. Recent 2025 advances include cutting-edge courses on generative AI security, federal red-teaming frameworks, and practical hands-on privacy-preserving ML systems.

**Keywords:** ai-security, adversarial-ml, model-security, privacy-preserving-ai, federated-learning, differential-privacy, adversarial-attacks, model-extraction, data-poisoning, trustworthy-ai, red-teaming, secure-generative-ai, prompt-injection, llm-security

**Skill Levels:** ğŸŸ¢ Beginner | ğŸŸ¡ Intermediate | ğŸ”´ Advanced

---

## ğŸ“š Topics Covered

- Adversarial machine learning attacks and defenses
- Model extraction and membership inference attacks
- Data poisoning and backdoor attacks
- Privacy-preserving machine learning
- Federated learning and differential privacy
- Secure multi-party computation
- Model robustness and adversarial training
- AI system vulnerabilities and threat modeling
- Prompt injection and generative AI security
- Red teaming and penetration testing for AI
- Prompt-level protections and secure inference
- Ethical hacking for AI systems

---

## ğŸ“ Courses & Tutorials

### ğŸŸ¢ Beginner-Friendly

- [Coursera: Securing AI Systems (2025)](https://www.coursera.org/learn/securing-ai-systems) â­ **NEW 2025** â€“ Hands-on course designed to safeguard machine learning applications against real-world threats. Explore vulnerabilities such as adversarial attacks, data poisoning, and model theft, then practice defense strategies through guided labs. Learn to build robust AI systems by exploring adversarial defense techniques and red-teaming practices. Includes SOC tools, cloud security, and incident response workflows. 4 modules with interactive labs and discussion prompts. (ğŸŸ¡ Intermediate)
  - ğŸ“– Access: Free audit available (Coursera platform)
  - ğŸ› ï¸ Hands-on: Yes (guided labs, red-teaming simulations)
  - â±ï¸ Duration: ~10 hours, self-paced
  - ğŸ¯ Topics: Adversarial attacks, data poisoning, model theft, SOC tools, incident response
  - [Tags: intermediate hands-on coursera adversarial-attacks red-teaming 2025]
  - [Verified: 2025-12-29]

- [Pluralsight: Adversarial AI - Detection and Defense](https://www.pluralsight.com/courses/adversarial-ai-detection-defense) â€“ Comprehensive course teaching essential techniques for identifying manipulated inputs and implementing robust defenses including adversarial training, input preprocessing, and model ensembles for production ML systems. (ğŸŸ¢ Beginner)
  - ğŸ“– Access: Free trial available (10-day)
  - ğŸ› ï¸ Hands-on: Yes, practical labs and exercises
  - ğŸ›ï¸ Authority: Pluralsight
  - [Tags: beginner adversarial-training detection defense hands-on 2025]
  - [Verified: 2025-12-16]

- [Mahara-Tech: AI Security Mastery - From Attacks to Defenses](https://maharatech.gov.eg/course/view.php?id=2373) â€“ Practical introduction to AI security threats covering adversarial machine learning, AI misuse, computer vision attacks with hands-on labs for detecting security threats and understanding defense strategies. (ğŸŸ¢ Beginner)
  - ğŸ“– Access: Fully open, free registration
  - ğŸ› ï¸ Hands-on: Yes, practical labs included
  - ğŸ›ï¸ Authority: Mahara-Tech (Egypt Digital Platform)
  - [Tags: beginner ai-security hands-on computer-vision defense 2025]
  - [Verified: 2025-12-16]

- [GitHub: Awesome Adversarial Machine Learning](https://github.com/man3kin3ko/awesome-adversarial-machine-learning) â€“ Curated collection of research papers, tools, defense mechanisms, and educational resources on ML security covering attack methodologies, robustness testing, and privacy-preserving techniques. (ğŸŸ¢ğŸŸ¡ All Levels)
  - ğŸ“– Access: Fully open GitHub repository
  - ğŸŒ Global: Community-maintained awesome list
  - [Tags: all-levels curated-list papers tools defenses 2025]
  - [Verified: 2025-12-16]

### ğŸŸ¡ Intermediate

- [Microsoft AI Red Teaming 101 Series (2025)](https://learn.microsoft.com/en-us/security/ai-red-team/training) â­ **OFFICIAL 2025** â€“ Comprehensive training series from Microsoft helping professionals secure generative AI systems against emerging threats. Dive into vulnerabilities, attack techniques (prompt injection, multi-turn attacks, jailbreaking), and defense strategies with real-world case studies from Microsoft's production AI security work. Part A covers vulnerabilities and attacks (prompt injection, single-turn and multi-turn attacks, filter evasion, encoding tricks). Part B covers defenses and mitigation strategies including Microsoft's Spotlighting methods (delimiting, data marking, encoding). Includes automation tools (PyRIT) for scaling red teaming efforts. (ğŸŸ¡ Intermediate)
  - ğŸ“– Access: Fully open, Microsoft Learn platform
  - ğŸ›ï¸ Authority: Microsoft (official security training)
  - ğŸ› ï¸ Hands-on: Yes (labs and real-world case studies)
  - â±ï¸ Duration: Multi-part series, self-paced
  - ğŸ¯ Topics: Prompt injection, jailbreaking, multi-turn attacks, spotlighting defense, PyRIT automation
  - [Tags: intermediate microsoft red-teaming generative-ai defense 2025]
  - [Verified: 2025-12-29]

- [Stanford CS 330i: AI Security Fundamentals (Fall 2025)](https://www.youtube.com/watch?v=5QmQ49BikQY) â­ **LIVE COURSE FALL 2025** â€“ Official Stanford course introducing security challenges of modern AI systems. Examine how vulnerabilities can be introduced during system architecture design, model development, training, and deployment. Explore attacks like prompt injection, adversarial inputs, data poisoning, and model extraction that exploit foundation models, retrieval-augmented systems, and AI agents. Learn about emerging defenses such as secure architectures, verifiable training, prompt-level protections, and gain deeper understanding of how to assess and improve AI system security. (ğŸŸ¡ Intermediate-Advanced)
  - ğŸ“– Access: Free on YouTube (official Stanford lectures)
  - ğŸ›ï¸ Authority: Stanford University (CS330i course, Fall 2025)
  - ğŸ¥ Format: Full lecture series, high-quality video
  - ğŸ¯ Topics: Foundation model security, RAG vulnerabilities, multi-agent AI security, prompt-level protections, secure architectures
  - [Tags: intermediate stanford ai-security course lectures 2025]
  - [Verified: 2025-12-29]

- [Pluralsight: Privacy-Preserving AI (2025)](https://www.pluralsight.com/courses/privacy-preserving-ai) â­ **2025 UPDATED** â€“ Course on implementing Privacy Enhancing Technologies (PETs) like Federated Learning, Differential Privacy, and Homomorphic Encryption. Balance data utility with privacy and compliance (GDPR). Learn to practically implement privacy-preserving technologies in real-world AI workflows, navigate computational overhead and data utility trade-offs while aligning with ethical AI principles. (ğŸŸ¡ Intermediate)
  - ğŸ“– Access: Free trial available (Pluralsight platform)
  - ğŸ› ï¸ Hands-on: Yes (implementation labs)
  - ğŸ¯ Topics: Differential Privacy, Federated Learning, Homomorphic Encryption, GDPR compliance
  - [Tags: intermediate pluralsight privacy differential-privacy federated-learning 2025]
  - [Verified: 2025-12-29]

- [HarmBench: Standardized Evaluation Framework for Red Teaming (Center for AI Safety)](https://github.com/centerforaisafety/HarmBench) â­ **NEW JAN 2026** â€“ Open-source evaluation framework for automated red teaming with 18+ evaluated attack methods and 33+ LLM defenses. Identifies desirable red teaming properties (coverage, clarity, efficiency), systematically designs benchmarks, and enables co-development of attacks and defenses. Includes highly efficient adversarial training method for LLM robustness across diverse attack types. Essential for understanding cutting-edge attack-defense dynamics. (ğŸŸ¡ Intermediate)
  - ğŸ“– Access: Fully open, GitHub repository
  - ğŸ›ï¸ Authority: Center for AI Safety (CAIS)
  - ğŸ› ï¸ Code: Python, Jupyter notebooks included
  - ğŸ¯ Topics: Red teaming evaluation, robust refusal, attack-defense co-development, LLM security
  - [Tags: intermediate red-teaming evaluation harmbench llm-security 2026]
  - [Verified: 2026-01-09]

- [AutoRedTeamer: Autonomous Red Teaming with Lifelong Attack Integration (arXiv)](https://arxiv.org/pdf/2502.05637.pdf) â­ **NEW MARCH 2025** â€“ Advanced framework for autonomous red teaming using dual-agent architecture: red-teaming agent generates test cases from high-level risk categories, and strategy proposer autonomously discovers new attacks via recent research analysis. Modular design enables continuous evolution with new attack vectors. Comprehensive, scalable approach to AI system security evaluation. (ğŸŸ¡ Intermediate-Advanced)
  - ğŸ“– Access: Fully open (arXiv preprint, March 2025)
  - ğŸ›ï¸ Authority: arXiv peer-reviewed research
  - ğŸ“„ Format: Technical paper with implementation details
  - ğŸ¯ Topics: Autonomous red teaming, attack discovery, lifelong learning, AI system robustness
  - [Tags: intermediate red-teaming autonomous-attacks research 2025]
  - [Verified: 2026-01-09]

- [FedLearn: Security & Adversarial AI](https://www.fedlearn.com/courses/security-adversarial-ai/) â€“ Essential online course providing introduction to vulnerabilities in ML systems with focus on DoD operations, covering data poisoning, Trojan insertion, backdoors, evasion attacks, and defense methods like adversarial training and differential privacy. (ğŸŸ¡ Intermediate)
  - ğŸ“– Access: Fully open, self-paced
  - ğŸ›ï¸ Authority: FedLearn (Defense-focused AI education)
  - ğŸ¯ Focus: DoD/military security applications
  - [Tags: intermediate defense-operations adversarial-attacks poisoning 2025]
  - [Verified: 2025-12-16]

- [DeepLearning.AI: Introduction to Federated Learning](https://www.deeplearning.ai/short-courses/intro-to-federated-learning/) â€“ Short course covering federated learning fundamentals including LLM pretraining steps, data preparation, model configuration, and performance assessment with privacy-preserving techniques. (ğŸŸ¡ Intermediate)
  - ğŸ“– Access: Free registration required
  - ğŸ›ï¸ Authority: DeepLearning.AI (Andrew Ng)
  - ğŸ› ï¸ Hands-on: Yes, practical exercises
  - â±ï¸ Duration: Short course format
  - [Tags: intermediate federated-learning privacy llm deeplearning-ai 2025]
  - [Verified: 2025-12-16]

- [Adversarial Machine Learning: Understanding and Preventing (Obsidian Security)](https://www.obsidiansecurity.com/blog/adversarial-machine-learning) â€“ Comprehensive enterprise guide to AI security threats including input manipulation, model extraction, and data poisoning attacks with practical mitigation strategies for production ML systems. (ğŸŸ¡ Intermediate)
  - ğŸ“– Access: Fully open blog article
  - ğŸ›ï¸ Authority: Obsidian Security (cybersecurity firm)
  - [Tags: intermediate adversarial-ml enterprise-security mitigation 2025]
  - [Verified: 2025-12-16]

- [SecureAI: Cybersecurity Training in AI (Loyola University Chicago)](https://secureai.cs.luc.edu) â€“ University workshop series introducing key concepts of privacy and security in AI systems covering privacy-preserving methods (differential privacy, federated learning), adversarial attacks, robustness, domain adaptation with various attack implementations and defense strategies. (ğŸŸ¡ Intermediate)
  - ğŸ“– Access: Fully open workshop materials
  - ğŸ›ï¸ Authority: Loyola University Chicago
  - [Tags: intermediate university workshop privacy federated-learning adversarial-robustness 2025]
  - [Verified: 2025-12-16]

### ğŸ”´ Advanced

- [AurÃ©lien Bellet: Privacy Preserving Machine Learning (Complete University Course)](https://researchers.lille.inria.fr/abellet/teaching/private_machine_learning_course.html) â­ **COMPREHENSIVE COURSE** â€“ Advanced course covering differential privacy theory and practice. Start with why classic anonymization fails and how ML models leak information. Dive into formal definition of differential privacy with key properties, design of differentially private algorithms in centralized settings, private empirical risk minimization with output perturbation and private SGD. Covers decentralized model with local differential privacy and federated learning. Includes complete lecture slides (6 lectures) and 5 practical Python sessions in Jupyter notebooks covering numeric/non-numeric queries, ERM, SGD, and local differential privacy. Rigorous mathematical foundation. (ğŸ”´ Advanced)
  - ğŸ“– Access: Fully open, official course materials
  - ğŸ›ï¸ Authority: Inria (French Institute for Research in Computer Science)
  - ğŸ› ï¸ Hands-on: Yes (5 Python practical sessions in Jupyter)
  - â±ï¸ Duration: Full university-level course
  - ğŸ¯ Topics: Differential privacy theory, private ERM, private SGD, local DP, federated learning, cryptographic foundations
  - [Tags: advanced differential-privacy federated-learning private-ml mathematical-foundations 2025]
  - [Verified: 2025-12-29]

- [Adversarial Machine Learning: Attacks, Defenses, and Open Challenges (arXiv 2025)](http://arxiv.org/pdf/2502.05637.pdf) â­ **FEBRUARY 2025 PAPER** â€“ Comprehensive analysis of adversarial machine learning covering vulnerabilities where adversaries manipulate inputs or training data. Formalizes defense mechanisms with mathematical rigor, discusses challenges of implementing robust solutions in adaptive threat models, and highlights open challenges in certified robustness. Addresses both evasion and poisoning attacks with practical defense implementation guidance. (ğŸ”´ Advanced)
  - ğŸ“– Access: Fully open (arXiv preprint, February 2025)
  - ğŸ›ï¸ Authority: arXiv peer-reviewed research
  - ğŸ“„ Format: 20+ page technical paper
  - [Tags: advanced adversarial-attacks mathematical-defenses robustness arxiv 2025]
  - [Verified: 2025-12-29]

- [TensorFlow Federated: Differential Privacy in Federated Learning](https://www.tensorflow.org/federated/tutorials/federated_learning_with_differential_privacy) â€“ Official TensorFlow tutorial demonstrating how to implement differential privacy techniques in federated learning systems with code examples and mathematical foundations for privacy guarantees. (ğŸ”´ Advanced)
  - ğŸ“– Access: Fully open, official documentation
  - ğŸ›ï¸ Authority: TensorFlow (Google)
  - ğŸ› ï¸ Hands-on: Yes, executable Colab notebooks
  - ğŸ’» Code: Python, TensorFlow Federated
  - [Tags: advanced differential-privacy federated-learning tensorflow tutorial 2025]
  - [Verified: 2025-12-16]

- [Comprehensive Review of Adversarial Attacks on Machine Learning (arXiv)](https://arxiv.org/pdf/2412.11384.pdf) â€“ Academic review providing comprehensive overview of adversarial attacks on AI/ML models, exploring attack types, techniques, business implications, mitigation strategies, and future research directions with Adversarial Robustness Toolbox insights. (ğŸ”´ Advanced)
  - ğŸ“– Access: Fully open (arXiv preprint)
  - ğŸ›ï¸ Authority: arXiv peer-reviewed research
  - ğŸ“„ Format: 40+ page academic paper
  - [Tags: advanced adversarial-attacks mitigation research comprehensive 2024]
  - [Verified: 2025-12-16]

- [NIST NCCoE: Adversarial Machine Learning](https://www.nccoe.nist.gov/ai/adversarial-machine-learning) â€“ Official NIST standards and guidance on designing secure ML algorithms resistant to adversarial manipulation, covering attack taxonomies, defense mechanisms, and best practices for trustworthy AI systems. (ğŸŸ¡ğŸ”´ Intermediate-Advanced)
  - ğŸ“– Access: Fully open, official government resource
  - ğŸ›ï¸ Authority: NIST (US National Institute of Standards and Technology)
  - [Tags: advanced nist standards government trustworthy-ai 2025]
  - [Verified: 2025-12-16]

- [Purple-teaming LLMs with Adversarial Defender Training (arXiv 2024)](https://arxiv.org/html/2407.01850v1) â­ **NEW JAN 2026** â€“ Novel approach to LLM safety combining red teaming (finding vulnerabilities) and blue teaming (building defenses) through purple-teaming methodology. Advanced adversarial training methods to enhance LLM robustness against jailbreaking attempts. Research-grade insights into modern LLM defense strategies. (ğŸ”´ Advanced)
  - ğŸ“– Access: Fully open (arXiv HTML version)
  - ğŸ›ï¸ Authority: arXiv peer-reviewed research
  - ğŸ“„ Format: Technical research paper
  - ğŸ¯ Topics: Purple teaming, adversarial training, LLM safety, red-blue teaming integration
  - [Tags: advanced purple-teaming llm-safety adversarial-training 2024]
  - [Verified: 2026-01-09]

---

## ğŸ“– Documentation & Guides

- [OpenMined: Privacy-Preserving Machine Learning Courses](https://courses.openmined.org) â€“ Series of free courses teaching how privacy is impacting every industry and how to build real-world products with privacy-preserving AI technology. Community-driven platform.
  - ğŸ“– Access: Fully open, free courses
  - [Verified: 2025-12-16]

- [ELSA-AI: Private Machine Learning Module](https://elsa-ai.eu/courses-and-tutorials/) â€“ Module focusing on principles and techniques of maintaining privacy in machine learning with key concepts like differential privacy, federated learning, and homomorphic encryption with practical applications and regulatory frameworks.
  - ğŸ“– Access: Fully open tutorial materials
  - [Verified: 2025-12-16]

- [Google Research: Distributed Differential Privacy for Federated Learning](https://research.google/blog/distributed-differential-privacy-for-federated-learning/) â€“ Technical blog post explaining distributed differential privacy techniques in federated learning systems with research insights from Google.
  - [Verified: 2025-12-16]

- [Data Science Salon: Federated Learning for Privacy-Preserving AI](https://roundtable.datascience.salon/federated-learning-for-privacy-preserving-ai-an-in-depth-exploration) â€“ In-depth exploration of federated learning's privacy advantages, mechanisms, and future directions for privacy-centric AI ecosystems.
  - [Verified: 2025-12-16]

- [Privacy-Preserving Machine Learning Guide (DeepSight)](https://deepsight.de/en/blog/privacy-preserving-machine-learning-guide/) â€“ Practical guide to building secure, compliant AI systems covering differential privacy, federated learning, homomorphic encryption with use cases and implementation guidance.
  - ğŸ“– Access: Fully open guide
  - [Verified: 2025-12-16]

---

## ğŸ› ï¸ Key Concepts & Tools

**Attack Types:**
- **Evasion Attacks** - Adversarial examples to fool models
- **Poisoning Attacks** - Corrupting training data
- **Backdoor Attacks** - Hidden triggers in models
- **Model Extraction** - Stealing model parameters
- **Membership Inference** - Detecting training data presence
- **Prompt Injection** - Manipulating LLM inputs
- **Prompt Jailbreaking** - Bypassing safety guardrails

**Defense Mechanisms:**
- **Adversarial Training** - Training on adversarial examples
- **Differential Privacy** - Privacy guarantees in outputs
- **Federated Learning** - Decentralized training
- **Robustness Certification** - Provable defenses
- **Input Sanitization** - Filtering malicious inputs
- **Red Teaming** - Simulated attacks to find vulnerabilities
- **Purple Teaming** - Integrated red + blue team defense
- **Prompt-Level Protections** - LLM input/output guardrails

**Tools & Frameworks:**
- **CleverHans** (TensorFlow) - Adversarial example library
- **Foolbox** - Adversarial attacks framework
- **ART (Adversarial Robustness Toolbox)** - IBM defense toolkit
- **TextAttack** - NLP adversarial attacks
- **PrivacyRaven** - Privacy attack testing
- **TensorFlow Federated** - Privacy-preserving ML framework
- **PyRIT** (Microsoft) - Red teaming automation
- **HarmBench** (CAIS) - Red teaming evaluation
- **OpenMined PySyft** - Encrypted ML framework

---

## ğŸ”— Related Resources

**See also:**
- [AI Ethics & Responsible AI](./ai-ethics.md) - Ethical considerations in AI security
- [MLOps](./mlops.md) - Secure model deployment practices
- [Explainable AI](./explainable-ai.md) - Model transparency and trust

**Cross-reference:**
- [Deep Learning & Neural Networks](./deep-learning-neural-networks.md) - Understanding model vulnerabilities
- [Natural Language Processing](./natural-language-processing.md) - NLP-specific security
- [Generative AI](./generative-ai.md) - LLM security and safety

---

## ğŸ¤ Contributing

Found a great free AI Security resource? We'd love to add it!

**To contribute, use this format:**
```
- [Resource Name](URL) â€“ Clear description highlighting value and what you'll learn. (Difficulty Level)
  - ğŸ“– Access: [access details]
  - ğŸ›ï¸ Authority: [Source/Organization]
  - [Tags: keyword1 keyword2 keyword3 year]
  - [Verified: YYYY-MM-DD]
```

**Ensure all resources are:**
- âœ… Completely free to access (no payment required)
- âœ… Openly available (no authentication barriers for core content)
- âœ… High-quality and educational
- âœ… Relevant to AI Security & Privacy
- âœ… From reputable sources (official docs, universities, established platforms)
- âœ… HTTP 200 verified before submission

---

**Last Updated:** January 9, 2026 | **Total Resources:** 19 (13 courses + 5 guides + tools)
**Last Link Validation:** January 9, 2026

**Keywords:** ai-security, adversarial-machine-learning, model-security, privacy-preserving-ai, federated-learning, differential-privacy, adversarial-attacks, model-extraction, data-poisoning, trustworthy-ai, robust-ml, secure-ai, nist-standards, tensorflow-federated, prompt-injection, red-teaming, purple-teaming, generative-ai-security, llm-safety, free-courses-2025-2026