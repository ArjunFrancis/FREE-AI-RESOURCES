# ğŸ” AI Security & Privacy

Security vulnerabilities, privacy-preserving techniques, and defensive strategies for protecting AI systems from adversarial attacks and ensuring responsible data handling.

## ğŸ“– Overview

AI Security focuses on protecting machine learning models and systems from malicious attacks, including adversarial examples, model extraction, data poisoning, and privacy violations. This emerging field addresses critical challenges in deploying trustworthy AI systems in production environments. Recent 2025-2026 advances include cutting-edge courses on generative AI security, federal red-teaming frameworks, and practical hands-on privacy-preserving ML systems.

**Keywords:** ai-security, adversarial-ml, model-security, privacy-preserving-ai, federated-learning, differential-privacy, adversarial-attacks, model-extraction, data-poisoning, trustworthy-ai, red-teaming, secure-generative-ai, prompt-injection, llm-security, threat-modeling, secure-ai-framework

**Skill Levels:** ğŸŸ¢ Beginner | ğŸŸ¡ Intermediate | ğŸ”´ Advanced

---

## ğŸ“š Topics Covered

- Adversarial machine learning attacks and defenses
- Model extraction and membership inference attacks
- Data poisoning and backdoor attacks
- Privacy-preserving machine learning
- Federated learning and differential privacy
- Secure multi-party computation
- Model robustness and adversarial training
- AI system vulnerabilities and threat modeling
- Prompt injection and generative AI security
- Red teaming and penetration testing for AI
- Prompt-level protections and secure inference
- Ethical hacking for AI systems
- Secure AI architecture design

---

## ğŸ“ Courses & Tutorials

### ğŸŸ¢ Beginner-Friendly

- [Coursera: Securing AI Systems (2025)](https://www.coursera.org/learn/securing-ai-systems) â˜… **NEW 2025** â€“ Hands-on course designed to safeguard machine learning applications against real-world threats. Explore vulnerabilities such as adversarial attacks, data poisoning, and model theft, then practice defense strategies through guided labs. Learn to build robust AI systems by exploring adversarial defense techniques and red-teaming practices. Includes SOC tools, cloud security, and incident response workflows. 4 modules with interactive labs and discussion prompts. (ğŸŸ¡ Intermediate)
  - ğŸ“– Access: Free audit available (Coursera platform)
  - ğŸ› ï¸ Hands-on: Yes (guided labs, red-teaming simulations)
  - â±ï¸ Duration: ~10 hours, self-paced
  - ğŸŒ Topics: Adversarial attacks, data poisoning, model theft, SOC tools, incident response
  - [Tags: intermediate hands-on coursera adversarial-attacks red-teaming 2025]
  - [Verified: 2025-12-29]

- [Pluralsight: Adversarial AI - Detection and Defense](https://www.pluralsight.com/courses/adversarial-ai-detection-defense) â€“ Comprehensive course teaching essential techniques for identifying manipulated inputs and implementing robust defenses including adversarial training, input preprocessing, and model ensembles for production ML systems. (ğŸŸ¢ Beginner)
  - ğŸ“– Access: Free trial available (10-day)
  - ğŸ› ï¸ Hands-on: Yes, practical labs and exercises
  - ğŸ›ï¸ Authority: Pluralsight
  - [Tags: beginner adversarial-training detection defense hands-on 2025]
  - [Verified: 2025-12-16]

- [Mahara-Tech: AI Security Mastery - From Attacks to Defenses](https://maharatech.gov.eg/course/view.php?id=2373) â€“ Practical introduction to AI security threats covering adversarial machine learning, AI misuse, computer vision attacks with hands-on labs for detecting security threats and understanding defense strategies. (ğŸŸ¢ Beginner)
  - ğŸ“– Access: Fully open, free registration
  - ğŸ› ï¸ Hands-on: Yes, practical labs included
  - ğŸ›ï¸ Authority: Mahara-Tech (Egypt Digital Platform)
  - [Tags: beginner ai-security hands-on computer-vision defense 2025]
  - [Verified: 2025-12-16]

- [GitHub: Awesome Adversarial Machine Learning](https://github.com/man3kin3ko/awesome-adversarial-machine-learning) â€“ Curated collection of research papers, tools, defense mechanisms, and educational resources on ML security covering attack methodologies, robustness testing, and privacy-preserving techniques. (ğŸŸ¢ğŸŸ¡ All Levels)
  - ğŸ“– Access: Fully open GitHub repository
  - ğŸŒ Global: Community-maintained awesome list
  - [Tags: all-levels curated-list papers tools defenses 2025]
  - [Verified: 2025-12-16]

- **[AI Security Training Lab: Hands-On OWASP LLM Top 10 (GitHub 2025)](https://github.com/citizenjosh/ai-security-training-lab)** ğŸŸ¢ Beginner - Comprehensive hands-on environment for learning AI security through real-world attack and defense scenarios. Based on OWASP Top 10 for LLM Applications with Docker-based labs covering prompt injection, insecure output handling, training data poisoning, model DoS, supply-chain vulnerabilities, sensitive information disclosure, insecure plugin design, excessive agency, overreliance, and model theft. Includes free open-source security tools (Guardrails AI, PromptInject, Cleanlab, SecretFlow, Opacus, TextAttack, RobustBench) and step-by-step tutorials for each vulnerability class.
  - ğŸ“– Access: Fully free (GitHub repository + Docker labs)
  - ğŸ›ï¸ Authority: Community-driven, OWASP-aligned
  - ğŸ› ï¸ Hands-on: Yes (Docker environments, real exploit scenarios)
  - ğŸ“Š Topics: OWASP LLM Top 10, prompt injection, adversarial attacks, model theft, ethical hacking, security tools
  - ğŸ“œ License: MIT
  - [Tags: beginner hands-on owasp llm-security docker labs prompt-injection 2025]
  - [Verified: 2026-02-18]

### ğŸŸ¡ Intermediate

- [Microsoft AI Red Teaming 101 Series (2025)](https://learn.microsoft.com/en-us/security/ai-red-team/training) â˜… **OFFICIAL 2025** â€“ Comprehensive training series from Microsoft helping professionals secure generative AI systems against emerging threats. Dive into vulnerabilities, attack techniques (prompt injection, multi-turn attacks, jailbreaking), and defense strategies with real-world case studies from Microsoft's production AI security work. Part A covers vulnerabilities and attacks (prompt injection, single-turn and multi-turn attacks, filter evasion, encoding tricks). Part B covers defenses and mitigation strategies including Microsoft's Spotlighting methods (delimiting, data marking, encoding). Includes automation tools (PyRIT) for scaling red teaming efforts. (ğŸŸ¡ Intermediate)
  - ğŸ“– Access: Fully open, Microsoft Learn platform
  - ğŸ›ï¸ Authority: Microsoft (official security training)
  - ğŸ› ï¸ Hands-on: Yes (labs and real-world case studies)
  - â±ï¸ Duration: Multi-part series, self-paced
  - ğŸŒ Topics: Prompt injection, jailbreaking, multi-turn attacks, spotlighting defense, PyRIT automation
  - [Tags: intermediate microsoft red-teaming generative-ai defense 2025]
  - [Verified: 2025-12-29]

- **[AI Red Teaming Explained (HackTheBox, 2025)](https://www.hackthebox.com/blog/ai-red-teaming-explained)** ğŸŸ¡ Intermediate â€“ Comprehensive guide to AI red-teaming methodologies covering adversarial simulation, adversarial testing, and capabilities testing. Explains how to identify AI vulnerabilities through structured attack scenarios. Includes real examples from Google and OpenAI demonstrating practical exploitation techniques. Part of HackTheBox's cybersecurity education platform with free intro-level courses and advanced training. 2025 security research perspective.
  - ğŸ“– Access: Fully free blog article
  - ğŸ›ï¸ Authority: HackTheBox (cybersecurity platform)
  - ğŸŒ Topics: Red teaming methodologies, adversarial simulation, attack structures, vulnerability assessment
  - ğŸŒ Global: Fully accessible worldwide
  - [Tags: intermediate red-teaming adversarial attack-scenarios hackthebox 2025]
  - [Verified: 2026-01-15]

- **[What AI Red Teaming Actually Looks Like: Methods & Real Examples (AyaData, 2025)](https://www.ayadata.ai/what-ai-red-teaming-actually-looks-like-methods-process-and-real-examples/)** ğŸŸ¡ Intermediate â€“ In-depth analysis of AI red-teaming processes with real examples from Google (adversarial examples) and OpenAI (jailbreak attempts). Covers manual + automated red-teaming techniques, attack vector development, execution phases, and defensive countermeasures. Current 2025 perspective on AI security threats and mitigation strategies with industry case studies. Practical insights into how leading AI companies approach security testing.
  - ğŸ“– Access: Fully free article
  - ğŸ›ï¸ Authority: AyaData (AI security research)
  - ğŸŒ Topics: Red-teaming processes, attack vectors, defense countermeasures, case studies, real-world examples
  - ğŸŒ Global: Fully accessible worldwide
  - [Tags: intermediate red-teaming adversarial-examples jailbreaks security-research 2025]
  - [Verified: 2026-01-15]

- **[AI Red Teaming: The Complete Guide (GitHub 2025-2026)](https://github.com/requie/AI-Red-Teaming-Guide)** ğŸŸ¡ Intermediate - Comprehensive open-source guide to adversarial testing and security evaluation of AI systems covering NIST AI RMF, OWASP GenAI Red Teaming, MITRE ATLAS, and CSA Agentic AI frameworks. Provides evidence-based methodologies from Microsoft's 100+ AI product red teams including attack vectors (prompt injection, jailbreaking, data poisoning, model extraction, membership inference), red teaming tools (PyRIT, DeepTeam, Garak, Giskard), real-world case studies (Microsoft SSRF, GPT-4 Base64, NIST ARIA), and practical implementation guides. Includes detailed threat modeling, manual and automated testing approaches, evaluation metrics (Attack Success Rate), and remediation strategies.
  - ğŸ“– Access: Fully open (GitHub repository)
  - ğŸ›ï¸ Authority: Community-driven, Microsoft research-based
  - ğŸ› ï¸ Hands-on: Yes (tool tutorials, code examples, practical methodologies)
  - ğŸ“Š Topics: Red teaming frameworks, attack vectors, prompt injection, jailbreaking, automated testing, PyRIT, threat modeling, real-world cases
  - ğŸŒ Global: Fully accessible worldwide
  - [Tags: intermediate red-teaming adversarial-testing nist-rmf owasp mitre-atlas prompt-injection tools-guide 2025]
  - [Verified: 2026-02-18]

- [Stanford CS 330i: AI Security Fundamentals (Fall 2025)](https://www.youtube.com/watch?v=5QmQ49BikQY) â˜… **LIVE COURSE FALL 2025** â€“ Official Stanford course introducing security challenges of modern AI systems. Examine how vulnerabilities can be introduced during system architecture design, model development, training, and deployment. Explore attacks like prompt injection, adversarial inputs, data poisoning, and model extraction that exploit foundation models, retrieval-augmented systems, and AI agents. Learn about emerging defenses such as secure architectures, verifiable training, prompt-level protections, and gain deeper understanding of how to assess and improve AI system security. (ğŸŸ¡ Intermediate-Advanced)
  - ğŸ“– Access: Free on YouTube (official Stanford lectures)
  - ğŸ›ï¸ Authority: Stanford University (CS330i course, Fall 2025)
  - ğŸ“º Format: Full lecture series, high-quality video
  - ğŸŒ Topics: Foundation model security, RAG vulnerabilities, multi-agent AI security, prompt-level protections, secure architectures
  - [Tags: intermediate stanford ai-security course lectures 2025]
  - [Verified: 2025-12-29]

- [Pluralsight: Privacy-Preserving AI (2025)](https://www.pluralsight.com/courses/privacy-preserving-ai) â˜… **2025 UPDATED** â€“ Course on implementing Privacy Enhancing Technologies (PETs) like Federated Learning, Differential Privacy, and Homomorphic Encryption. Balance data utility with privacy and compliance (GDPR). Learn to practically implement privacy-preserving technologies in real-world AI workflows, navigate computational overhead and data utility trade-offs while aligning with ethical AI principles. (ğŸŸ¡ Intermediate)
  - ğŸ“– Access: Free trial available (Pluralsight platform)
  - ğŸ› ï¸ Hands-on: Yes (implementation labs)
  - ğŸŒ Topics: Differential Privacy, Federated Learning, Homomorphic Encryption, GDPR compliance
  - [Tags: intermediate pluralsight privacy differential-privacy federated-learning 2025]
  - [Verified: 2025-12-29]

- [HarmBench: Standardized Evaluation Framework for Red Teaming (Center for AI Safety)](https://github.com/centerforaisafety/HarmBench) â˜… **NEW JAN 2026** â€“ Open-source evaluation framework for automated red teaming with 18+ evaluated attack methods and 33+ LLM defenses. Identifies desirable red teaming properties (coverage, clarity, efficiency), systematically designs benchmarks, and enables co-development of attacks and defenses. Includes highly efficient adversarial training method for LLM robustness across diverse attack types. Essential for understanding cutting-edge attack-defense dynamics. (ğŸŸ¡ Intermediate)
  - ğŸ“– Access: Fully open, GitHub repository
  - ğŸ›ï¸ Authority: Center for AI Safety (CAIS)
  - ğŸ› ï¸ Code: Python, Jupyter notebooks included
  - ğŸŒ Topics: Red teaming evaluation, robust refusal, attack-defense co-development, LLM security
  - [Tags: intermediate red-teaming evaluation harmbench llm-security 2026]
  - [Verified: 2026-01-09]

- [AutoRedTeamer: Autonomous Red Teaming with Lifelong Attack Integration (arXiv)](https://arxiv.org/pdf/2502.05637.pdf) â˜… **NEW MARCH 2025** â€“ Advanced framework for autonomous red teaming using dual-agent architecture: red-teaming agent generates test cases from high-level risk categories, and strategy proposer autonomously discovers new attacks via recent research analysis. Modular design enables continuous evolution with new attack vectors. Comprehensive, scalable approach to AI system security evaluation. (ğŸŸ¡ Intermediate-Advanced)
  - ğŸ“– Access: Fully open (arXiv preprint, March 2025)
  - ğŸ›ï¸ Authority: arXiv peer-reviewed research
  - ğŸ“„ Format: Technical paper with implementation details
  - ğŸŒ Topics: Autonomous red teaming, attack discovery, lifelong learning, AI system robustness
  - [Tags: intermediate red-teaming autonomous-attacks research 2025]
  - [Verified: 2026-01-09]

- [FedLearn: Security & Adversarial AI](https://www.fedlearn.com/courses/security-adversarial-ai/) â€“ Essential online course providing introduction to vulnerabilities in ML systems with focus on DoD operations, covering data poisoning, Trojan insertion, backdoors, evasion attacks, and defense methods like adversarial training and differential privacy. (ğŸŸ¡ Intermediate)
  - ğŸ“– Access: Fully open, self-paced
  - ğŸ›ï¸ Authority: FedLearn (Defense-focused AI education)
  - ğŸŒ Focus: DoD/military security applications
  - [Tags: intermediate defense-operations adversarial-attacks poisoning 2025]
  - [Verified: 2025-12-16]

- [DeepLearning.AI: Introduction to Federated Learning](https://www.deeplearning.ai/short-courses/intro-to-federated-learning/) â€“ Short course covering federated learning fundamentals including LLM pretraining steps, data preparation, model configuration, and performance assessment with privacy-preserving techniques. (ğŸŸ¡ Intermediate)
  - ğŸ“– Access: Free registration required
  - ğŸ›ï¸ Authority: DeepLearning.AI (Andrew Ng)
  - ğŸ› ï¸ Hands-on: Yes, practical exercises
  - â±ï¸ Duration: Short course format
  - [Tags: intermediate federated-learning privacy llm deeplearning-ai 2025]
  - [Verified: 2025-12-16]

- [Adversarial Machine Learning: Understanding and Preventing (Obsidian Security)](https://www.obsidiansecurity.com/blog/adversarial-machine-learning) â€“ Comprehensive enterprise guide to AI security threats including input manipulation, model extraction, and data poisoning attacks with practical mitigation strategies for production ML systems. (ğŸŸ¡ Intermediate)
  - ğŸ“– Access: Fully open blog article
  - ğŸ›ï¸ Authority: Obsidian Security (cybersecurity firm)
  - [Tags: intermediate adversarial-ml enterprise-security mitigation 2025]
  - [Verified: 2025-12-16]

- [SecureAI: Cybersecurity Training in AI (Loyola University Chicago)](https://secureai.cs.luc.edu) â€“ University workshop series introducing key concepts of privacy and security in AI systems covering privacy-preserving methods (differential privacy, federated learning), adversarial attacks, robustness, domain adaptation with various attack implementations and defense strategies. (ğŸŸ¡ Intermediate)
  - ğŸ“– Access: Fully open workshop materials
  - ğŸ›ï¸ Authority: Loyola University Chicago
  - [Tags: intermediate university workshop privacy federated-learning adversarial-robustness 2025]
  - [Verified: 2025-12-16]

### ğŸ”´ Advanced

- **[Google Secure AI Framework (SAIF): Official Security Guidelines](https://secureai.google/)** ğŸ”´ Advanced â€“ Google's official framework for building secure AI systems, available in full open-source format via GitHub. Provides comprehensive threat modeling, red-teaming practices, privacy protection strategies, and compliance frameworks for enterprise ML security. Includes detailed implementation guidelines, case studies, and threat catalogs. Authoritative source from Google's AI Security team for designing production-grade secure AI architectures.
  - ğŸ“– Access: Fully free, official framework
  - ğŸ›ï¸ Authority: Google AI Security Team (official source)
  - ğŸ› ï¸ Format: Interactive documentation + GitHub repository
  - ğŸ“Š Topics: Threat modeling, red-teaming practices, privacy strategies, compliance, secure architecture
  - ğŸ“œ License: Open source
  - ğŸŒ Global: Fully accessible worldwide
  - [Tags: advanced google-framework threat-modeling red-teaming privacy secure-architecture 2025]
  - [Verified: 2026-01-15]

- [AurÃ©lien Bellet: Privacy Preserving Machine Learning (Complete University Course)](https://researchers.lille.inria.fr/abellet/teaching/private_machine_learning_course.html) â˜… **COMPREHENSIVE COURSE** â€“ Advanced course covering differential privacy theory and practice. Start with why classic anonymization fails and how ML models leak information. Dive into formal definition of differential privacy with key properties, design of differentially private algorithms in centralized settings, private empirical risk minimization with output perturbation and private SGD. Covers decentralized model with local differential privacy and federated learning. Includes complete lecture slides (6 lectures) and 5 practical Python sessions in Jupyter notebooks covering numeric/non-numeric queries, ERM, SGD, and local differential privacy. Rigorous mathematical foundation. (ğŸ”´ Advanced)
  - ğŸ“– Access: Fully open, official course materials
  - ğŸ›ï¸ Authority: Inria (French Institute for Research in Computer Science)
  - ğŸ› ï¸ Hands-on: Yes (5 Python practical sessions in Jupyter)
  - â±ï¸ Duration: Full university-level course
  - ğŸŒ Topics: Differential privacy theory, private ERM, private SGD, local DP, federated learning, cryptographic foundations
  - [Tags: advanced differential-privacy federated-learning private-ml mathematical-foundations 2025]
  - [Verified: 2025-12-29]

- [Adversarial Machine Learning: Attacks, Defenses, and Open Challenges (arXiv 2025)](http://arxiv.org/pdf/2502.05637.pdf) â˜… **FEBRUARY 2025 PAPER** â€“ Comprehensive analysis of adversarial machine learning covering vulnerabilities where adversaries manipulate inputs or training data. Formalizes defense mechanisms with mathematical rigor, discusses challenges of implementing robust solutions in adaptive threat models, and highlights open challenges in certified robustness. Addresses both evasion and poisoning attacks with practical defense implementation guidance. (ğŸ”´ Advanced)
  - ğŸ“– Access: Fully open (arXiv preprint, February 2025)
  - ğŸ›ï¸ Authority: arXiv peer-reviewed research
  - ğŸ“„ Format: 20+ page technical paper
  - [Tags: advanced adversarial-attacks mathematical-defenses robustness arxiv 2025]
  - [Verified: 2025-12-29]

- **[TensorFlow Privacy: Differential Privacy Tutorial (Official Google)](https://www.tensorflow.org/responsible_ai/privacy/tutorials/classification_privacy)** ğŸ”´ Advanced - Official TensorFlow tutorial demonstrating differential privacy implementation in machine learning using DP-SGD (Differentially Private Stochastic Gradient Descent). Covers modifying gradients for privacy guarantees, gradient clipping and noise addition techniques, privacy budget tracking with epsilon/delta parameters, and measuring privacy-utility tradeoffs. Includes complete executable Colab notebooks with real datasets, step-by-step code examples for wrapping optimizers (SGD, Adam) with differential privacy, hyperparameter tuning for privacy-preserving models, and analysis tools for quantifying privacy guarantees. Essential resource for implementing production-ready private ML systems.
  - ğŸ“– Access: Fully open (official TensorFlow documentation)
  - ğŸ›ï¸ Authority: TensorFlow / Google (official source)
  - ğŸ› ï¸ Hands-on: Yes (executable Colab notebooks, complete code)
  - ğŸ’ª Code: Python, TensorFlow 2.x, TensorFlow Privacy library
  - ğŸ“Š Topics: DP-SGD, gradient clipping, noise mechanisms, privacy budget, epsilon-delta analysis
  - [Tags: advanced differential-privacy tensorflow dp-sgd tutorial hands-on google 2025]
  - [Verified: 2026-02-18]

- **[IBM Differential Privacy Library (Diffprivlib) - Open Source](https://github.com/IBM/differential-privacy-library)** ğŸ”´ Advanced - Production-grade open-source Python library from IBM Research providing comprehensive toolkit for differential privacy in machine learning. Includes four major components: (1) Privacy mechanisms (building blocks for DP algorithms with expert-level control), (2) ML models with built-in DP (clustering, classification, regression, dimensionality reduction, preprocessing), (3) Differentially private data analysis tools (histograms compatible with NumPy), and (4) BudgetAccountant for tracking privacy loss using advanced composition techniques. Drop-in replacements for scikit-learn models with privacy guarantees. Extensively documented with tutorials, examples, and mathematical foundations.
  - ğŸ“– Access: Fully free, open source (MIT License)
  - ğŸ›ï¸ Authority: IBM Research (official library)
  - ğŸ› ï¸ Hands-on: Yes (Python library, extensive examples, Jupyter notebooks)
  - ğŸ’ª Code: Python, scikit-learn compatible API
  - ğŸ“Š Topics: Differential privacy mechanisms, private ML models, privacy budgets, advanced composition, scikit-learn integration
  - ğŸ“œ License: MIT
  - [Tags: advanced differential-privacy open-source ibm python library production-ready 2025]
  - [Verified: 2026-02-18]

- [TensorFlow Federated: Differential Privacy in Federated Learning](https://www.tensorflow.org/federated/tutorials/federated_learning_with_differential_privacy) â€“ Official TensorFlow tutorial demonstrating how to implement differential privacy techniques in federated learning systems with code examples and mathematical foundations for privacy guarantees. (ğŸ”´ Advanced)
  - ğŸ“– Access: Fully open, official documentation
  - ğŸ›ï¸ Authority: TensorFlow (Google)
  - ğŸ› ï¸ Hands-on: Yes, executable Colab notebooks
  - ğŸ’ª Code: Python, TensorFlow Federated
  - [Tags: advanced differential-privacy federated-learning tensorflow tutorial 2025]
  - [Verified: 2025-12-16]

- [Comprehensive Review of Adversarial Attacks on Machine Learning (arXiv)](https://arxiv.org/pdf/2412.11384.pdf) â€“ Academic review providing comprehensive overview of adversarial attacks on AI/ML models, exploring attack types, techniques, business implications, mitigation strategies, and future research directions with Adversarial Robustness Toolbox insights. (ğŸ”´ Advanced)
  - ğŸ“– Access: Fully open (arXiv preprint)
  - ğŸ›ï¸ Authority: arXiv peer-reviewed research
  - ğŸ“„ Format: 40+ page academic paper
  - [Tags: advanced adversarial-attacks mitigation research comprehensive 2024]
  - [Verified: 2025-12-16]

- [NIST NCCoE: Adversarial Machine Learning](https://www.nccoe.nist.gov/ai/adversarial-machine-learning) â€“ Official NIST standards and guidance on designing secure ML algorithms resistant to adversarial manipulation, covering attack taxonomies, defense mechanisms, and best practices for trustworthy AI systems. (ğŸŸ¡ğŸ”´ Intermediate-Advanced)
  - ğŸ“– Access: Fully open, official government resource
  - ğŸ›ï¸ Authority: NIST (US National Institute of Standards and Technology)
  - [Tags: advanced nist standards government trustworthy-ai 2025]
  - [Verified: 2025-12-16]

- [Purple-teaming LLMs with Adversarial Defender Training (arXiv 2024)](https://arxiv.org/html/2407.01850v1) â˜… **NEW JAN 2026** â€“ Novel approach to LLM safety combining red teaming (finding vulnerabilities) and blue teaming (building defenses) through purple-teaming methodology. Advanced adversarial training methods to enhance LLM robustness against jailbreaking attempts. Research-grade insights into modern LLM defense strategies. (ğŸ”´ Advanced)
  - ğŸ“– Access: Fully open (arXiv HTML version)
  - ğŸ›ï¸ Authority: arXiv peer-reviewed research
  - ğŸ“„ Format: Technical research paper
  - ğŸŒ Topics: Purple teaming, adversarial training, LLM safety, red-blue teaming integration
  - [Tags: advanced purple-teaming llm-safety adversarial-training 2024]
  - [Verified: 2026-01-09]

---

## ğŸ“– Documentation & Guides

- [OpenMined: Privacy-Preserving Machine Learning Courses](https://courses.openmined.org) â€“ Series of free courses teaching how privacy is impacting every industry and how to build real-world products with privacy-preserving AI technology. Community-driven platform.
  - ğŸ“– Access: Fully open, free courses
  - [Verified: 2025-12-16]

- [ELSA-AI: Private Machine Learning Module](https://elsa-ai.eu/courses-and-tutorials/) â€“ Module focusing on principles and techniques of maintaining privacy in machine learning with key concepts like differential privacy, federated learning, and homomorphic encryption with practical applications and regulatory frameworks.
  - ğŸ“– Access: Fully open tutorial materials
  - [Verified: 2025-12-16]

- [Google Research: Distributed Differential Privacy for Federated Learning](https://research.google/blog/distributed-differential-privacy-for-federated-learning/) â€“ Technical blog post explaining distributed differential privacy techniques in federated learning systems with research insights from Google.
  - [Verified: 2025-12-16]

- [Data Science Salon: Federated Learning for Privacy-Preserving AI](https://roundtable.datascience.salon/federated-learning-for-privacy-preserving-ai-an-in-depth-exploration) â€“ In-depth exploration of federated learning's privacy advantages, mechanisms, and future directions for privacy-centric AI ecosystems.
  - [Verified: 2025-12-16]

- [Privacy-Preserving Machine Learning Guide (DeepSight)](https://deepsight.de/en/blog/privacy-preserving-machine-learning-guide/) â€“ Practical guide to building secure, compliant AI systems covering differential privacy, federated learning, homomorphic encryption with use cases and implementation guidance.
  - ğŸ“– Access: Fully open guide
  - [Verified: 2025-12-16]

---

## ğŸ› ï¸ Key Concepts & Tools

**Attack Types:**
- **Evasion Attacks** - Adversarial examples to fool models
- **Poisoning Attacks** - Corrupting training data
- **Backdoor Attacks** - Hidden triggers in models
- **Model Extraction** - Stealing model parameters
- **Membership Inference** - Detecting training data presence
- **Prompt Injection** - Manipulating LLM inputs
- **Prompt Jailbreaking** - Bypassing safety guardrails

**Defense Mechanisms:**
- **Adversarial Training** - Training on adversarial examples
- **Differential Privacy** - Privacy guarantees in outputs
- **Federated Learning** - Decentralized training
- **Robustness Certification** - Provable defenses
- **Input Sanitization** - Filtering malicious inputs
- **Red Teaming** - Simulated attacks to find vulnerabilities
- **Purple Teaming** - Integrated red + blue team defense
- **Prompt-Level Protections** - LLM input/output guardrails
- **Threat Modeling** - Systematic vulnerability assessment

**Tools & Frameworks:**
- **CleverHans** (TensorFlow) - Adversarial example library
- **Foolbox** - Adversarial attacks framework
- **ART (Adversarial Robustness Toolbox)** - IBM defense toolkit
- **TextAttack** - NLP adversarial attacks
- **PrivacyRaven** - Privacy attack testing
- **TensorFlow Federated** - Privacy-preserving ML framework
- **TensorFlow Privacy** - Differential privacy for TensorFlow
- **IBM Diffprivlib** - Differential privacy library
- **PyRIT** (Microsoft) - Red teaming automation
- **HarmBench** (CAIS) - Red teaming evaluation
- **OpenMined PySyft** - Encrypted ML framework
- **Google SAIF** - Secure AI framework
- **Guardrails AI** - LLM output validation
- **PromptInject** - Prompt injection testing

---

## ğŸ”— Related Resources

**See also:**
- [AI Ethics & Responsible AI](./ai-ethics.md) - Ethical considerations in AI security
- [MLOps](./mlops.md) - Secure model deployment practices
- [Explainable AI](./explainable-ai.md) - Model transparency and trust

**Cross-reference:**
- [Deep Learning & Neural Networks](./deep-learning-neural-networks.md) - Understanding model vulnerabilities
- [Natural Language Processing](./natural-language-processing.md) - NLP-specific security
- [Generative AI](./generative-ai.md) - LLM security and safety

---

## ğŸ¤ Contributing

Found a great free AI Security resource? We'd love to add it!

**To contribute, use this format:**
```
- [Resource Name](URL) â€“ Clear description highlighting value and what you'll learn. (Difficulty Level)
  - ğŸ“– Access: [access details]
  - ğŸ›ï¸ Authority: [Source/Organization]
  - [Tags: keyword1 keyword2 keyword3 year]
  - [Verified: YYYY-MM-DD]
```

**Ensure all resources are:**
- âœ… Completely free to access (no payment required)
- âœ… Openly available (no authentication barriers for core content)
- âœ… High-quality and educational
- âœ… Relevant to AI Security & Privacy
- âœ… From reputable sources (official docs, universities, established platforms)
- âœ… HTTP 200 verified before submission

---

**Last Updated:** February 18, 2026 | **Total Resources:** 26 (+4 new)
**Last Link Validation:** February 18, 2026

**Keywords:** ai-security, adversarial-machine-learning, model-security, privacy-preserving-ai, federated-learning, differential-privacy, adversarial-attacks, model-extraction, data-poisoning, trustworthy-ai, robust-ml, secure-ai, threat-modeling, google-saif, tensorflow-federated, tensorflow-privacy, ibm-diffprivlib, prompt-injection, red-teaming, purple-teaming, generative-ai-security, llm-safety, owasp-llm-top-10, hackthebox, hands-on-labs, free-courses-2025-2026