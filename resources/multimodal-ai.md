# ğŸŒˆ Multimodal AI

AI systems that process and integrate multiple types of data (text, images, audio, video) for enhanced understanding, generation, and cross-modal reasoning.

## ğŸ“– Overview

Multimodal AI represents the next frontier in artificial intelligence, enabling systems to understand and generate content across different modalities. These models can process text alongside images, audio, video, and other data types to create more comprehensive AI applications that mirror human multi-sensory perception.

**Keywords:** multimodal-ai, vision-language-models, cross-modal-learning, multimodal-generation, clip, dall-e, flamingo, multimodal-transformers, audio-visual-learning, multimodal-llms, transformers, huggingface

**Skill Levels:** ğŸŸ¢ Beginner | ğŸŸ¡ Intermediate | ğŸ”´ Advanced

---

## ğŸ“š Topics Covered

- Vision-language models (CLIP, DALL-E, Flamingo, GPT-4V)
- Audio-text integration (Whisper, AudioLM, MusicLM)
- Video understanding and generation
- Cross-modal retrieval and alignment
- Multimodal transformers and attention mechanisms
- Image captioning and Visual Question Answering (VQA)
- Text-to-image and text-to-video generation
- Multimodal fusion techniques
- Audio-visual learning
- Vision-language model architectures and design patterns
- Medical multimodal AI and clinical applications
- Multimodal representation learning
- Vision-Language-Action (VLA) models for robotics

---

## ğŸ“ Courses & Tutorials

### ğŸŸ¢ Beginner-Friendly

- **[Vision-Language Models Free Course - OpenCV University](https://opencv.org/university/free-courses/)** - Free Vision-Language Models bootcamp course covering CLIP for zero-shot image classification, Qwen2.5-VL for image captioning and object detection with detailed hands-on Python tutorials. Learn to implement vision-language models from beginner to advanced level with practical code examples.
  - ğŸ“– **Access:** Fully free, no login required  
  - ğŸ›ï¸ **Authority:** OpenCV (industry standard)  
  - ğŸ¯ **Level:** ğŸŸ¢ Beginner  
  - ğŸ› ï¸ **Hands-on:** Yes (Python code examples and tutorials)  
  - **Topics:** CLIP, Qwen2.5-VL, zero-shot classification, image captioning, object detection  
  - [Tags: `beginner` `vlm-bootcamp` `clip` `vision-language` `hands-on` `opencv` `python` `2025`]

- [Simplilearn: Free Multi-Modal LLMs Course](https://www.simplilearn.com/free-multimodal-llm-course-skillup) â€“ Beginner-friendly introduction to multimodal large language models covering text-image integration, cross-modal understanding, and practical applications with free certificate upon completion.
  - ğŸ“– Access: Free account required, certificate included
  - â±ï¸ Duration: Self-paced
  - ğŸ“œ Certificate: Free certificate available
  - [Tags: beginner multimodal-llms vision-language introduction 2025]

- **[Open Source Models with Hugging Face (DeepLearning.AI)](https://www.deeplearning.ai/short-courses/open-source-models-hugging-face/)** - 2-hour comprehensive course on finding and using open-source models from Hugging Face Hub for multimodal tasks (text, audio, image, multimodal). Learn to filter models by task, rankings, and memory requirements. Write minimal code using the transformers library to perform multimodal tasks. Deploy apps with Gradio and Hugging Face Spaces.
  - ğŸ“– **Access:** 100% free  
  - â±ï¸ **Duration:** 2 hours (16 video lessons, 13 code examples)  
  - ğŸ› ï¸ **Hands-on:** Yes (practical coding exercises)  
  - ğŸ›ï¸ **Source:** DeepLearning.AI + Hugging Face Partnership  
  - **Topics:** Model selection, Transformers library, Text/audio/image/multimodal tasks, Gradio deployment, Hugging Face Spaces  
  - [Tags: `beginner` `open-source-models` `huggingface` `transformers` `multimodal` `deployment` `deeplearning-ai` `2025`]

- **[Hugging Face Tutorial for Beginners - YouTube](https://www.youtube.com/watch?v=3xLTD5wSBEs)** - Beginner-friendly 11-minute tutorial covering HuggingFace platform essentials. Learn to explore pre-trained models, run NLP and multimodal tasks, use the transformers library effectively, and build real projects with step-by-step guidance.
  - ğŸ“– **Access:** Fully free on YouTube  
  - â±ï¸ **Duration:** 11 minutes (beginner-friendly)  
  - ğŸ¯ **Level:** ğŸŸ¢ğŸŸ¡ Beginner-Intermediate  
  - ğŸ› ï¸ **Hands-on:** Yes (practical walkthrough)  
  - **Topics:** HuggingFace platform, model exploration, transformers library, multimodal tasks  
  - [Tags: `beginner` `huggingface-tutorial` `platform-guide` `transformers-library` `youtube` `practical` `2025`]

### ğŸŸ¡ Intermediate

- **[Modern AI Models for Vision and Multimodal Understanding - Coursera](https://www.coursera.org/learn/modern-ai-models-vision-multimodal-understanding)** - Advanced university course covering SVMs, RNNs, Vision Transformers (ViT), CLIP for multimodal learning, and diffusion models. Includes mathematical foundations, practical implementations, and 19 hands-on assignments with coding exercises.
  - ğŸ“– **Access:** Free audit available on Coursera (certificate optional paid)  
  - ğŸ›ï¸ **Authority:** University course  
  - ğŸ¯ **Level:** ğŸŸ¡ğŸ”´ Intermediate-Advanced  
  - ğŸ› ï¸ **Hands-on:** Yes (19 assignments, coding exercises)  
  - **Topics:** Vision Transformers, CLIP, diffusion models, multimodal learning, mathematical foundations  
  - [Tags: `intermediate-advanced` `vision-transformers` `clip` `diffusion-models` `coursera` `university-course` `hands-on` `2025`]

- **[Vision-Language Models on the Edge - Embedded Vision Summit YouTube](https://www.youtube.com/watch?v=l_eJaNX0Pbo)** - Tutorial from Embedded Vision Summit covering deployment of vision-language models on edge devices using HuggingFace SmolVLM. Learn training processes, alignment techniques, and optimization strategies for resource-constrained environments.
  - ğŸ“– **Access:** Fully free on YouTube  
  - ğŸ›ï¸ **Authority:** Embedded Vision Summit (industry conference)  
  - ğŸ¯ **Level:** ğŸŸ¡ Intermediate  
  - **Topics:** Edge deployment, SmolVLM, optimization, resource constraints, VLM training  
  - [Tags: `intermediate` `edge-deployment` `optimization` `vlm-training` `huggingface` `youtube` `embedded-systems` `2025`]

- **[CLIP Model Documentation - Hugging Face](https://huggingface.co/docs/transformers/en/model_doc/clip)** - Official HuggingFace documentation for CLIP (Contrastive Language-Image Pre-training). Comprehensive guide explaining architecture, zero-shot transfer learning, image and text encoders, joint embedding space, and implementation with code examples.
  - ğŸ“– **Access:** Fully free, no login required  
  - ğŸ›ï¸ **Authority:** HuggingFace (Official)  
  - ğŸ¯ **Level:** ğŸŸ¡ Intermediate  
  - ğŸ› ï¸ **Hands-on:** Yes (code examples included)  
  - **Topics:** CLIP architecture, zero-shot learning, encoders, joint embeddings, implementation  
  - [Tags: `intermediate` `clip` `official-docs` `huggingface` `architecture` `zero-shot` `code-examples` `2025`]

- **[Multimodal LLMs: Vision & Language Integration - ProjectPro](https://www.projectpro.io/article/multimodal-llms/1054)** - Comprehensive article explaining how multimodal LLMs integrate text and images. Covers architecture components, training techniques (contrastive learning), CLIP and BLIP-2 models, and real-world applications in image captioning and Visual Question Answering (VQA).
  - ğŸ“– **Access:** Fully free  
  - ğŸ¯ **Level:** ğŸŸ¡ Intermediate  
  - **Focus:** Theory and practical understanding, architecture, training techniques, applications  
  - **Topics:** Multimodal LLM architecture, contrastive learning, CLIP, BLIP-2, image captioning, VQA  
  - [Tags: `intermediate` `multimodal-llms` `architecture` `training-techniques` `clip` `blip-2` `applications` `2025`]

- **[Best Open Source Multimodal Vision Models in 2025 - Koyeb](https://www.koyeb.com/blog/best-multimodal-vision-models-in-2025)** - Comparative guide of top open-source multimodal vision models released in 2025. Covers Gemma 3, Qwen 2.5 VL 72B Instruct, Pixtral, Phi 4 Multimodal, DeepSeek Janus Pro with technical details, architecture descriptions, and deployment recommendations for serverless GPUs.
  - ğŸ“– **Access:** Fully free  
  - ğŸ¯ **Level:** ğŸŸ¡ Intermediate  
  - **Focus:** 2025 models, benchmarks, technical comparison, deployment strategies  
  - **Models Covered:** Gemma 3, Qwen 2.5 VL, Pixtral, Phi 4 Multimodal, DeepSeek Janus Pro  
  - [Tags: `intermediate` `2025-models` `comparative-guide` `deployment` `benchmarks` `koyeb` `serverless-gpu` `2025`]

- **[Ultimate Guide - Best Open Source Multimodal Models 2025 - SiliconFlow](https://www.siliconflow.com/articles/en/best-open-source-multimodal-models-2025)** - Comprehensive guide comparing state-of-the-art open-source multimodal models in 2025. Detailed analysis of GLM-4.5V with 3D-RoPE and Thinking Mode, GLM-4.1V-9B-Thinking for efficiency, and Qwen2.5-VL 72B with advanced vision capabilities. Includes benchmarks, architectural innovations, and use case recommendations.
  - ğŸ“– **Access:** Fully free  
  - ğŸ¯ **Level:** ğŸŸ¡ Intermediate  
  - **Focus:** State-of-the-art models, benchmarks, architectural innovations  
  - **Models:** GLM-4.5V (106B parameters, MoE), GLM-4.1V-9B-Thinking (efficient), Qwen2.5-VL (vision capabilities)  
  - **Key Tech:** 3D-RoPE for spatial reasoning, Mixture-of-Experts (MoE) architecture, Thinking Mode  
  - [Tags: `intermediate` `2025-models` `glm-4.5v` `qwen2.5-vl` `moe-architecture` `3d-rope` `benchmarks` `siliconflow` `2025`]

- **[LLaVA: Large Language and Vision Assistant Architecture Deep Dive - LearnOpenCV](https://learnopencv.com/llava-training-a-visual-assistant/)** - Comprehensive technical tutorial on LLaVA architecture, training methodology, and implementation details. Covers CLIP vision encoder, MLP projector, two-stage training strategy (feature alignment and instruction tuning), dataset curation, and practical training optimizations on GPU clusters.
  - ğŸ“– **Access:** Fully free, comprehensive guide  
  - ğŸ›ï¸ **Authority:** LearnOpenCV (industry education)  
  - ğŸ¯ **Level:** ğŸŸ¡ğŸ”´ Intermediate-Advanced  
  - ğŸ› ï¸ **Hands-on:** Architecture and training methodology  
  - **Topics:** LLaVA-1.5 architecture, CLIP ViT, MLP projector, two-stage training, dataset curation, optimization  
  - **Key Insights:** Feature alignment strategy, instruction fine-tuning, Vicuna LLM integration, training efficiency  
  - [Tags: `intermediate-advanced` `llava` `architecture` `training` `vision-encoder` `learnopencv` `2025`]

- [Coursera: Build Multimodal Generative AI Applications (IBM)](https://www.coursera.org/learn/build-multimodal-generative-ai-applications) â€“ Hands-on course using IBM watsonx.ai to build multimodal applications integrating Granite, Llama 3, Whisper, and DALLÂ·3 for text, image, and audio generation with 3-week project-based learning.
  - ğŸ“– Access: Free audit available (certificate optional paid)
  - ğŸ›ï¸ Authority: IBM + Coursera
  - â±ï¸ Duration: 3 weeks, 2-3 hours/week
  - ğŸ› ï¸ Hands-on: Yes, with IBM watsonx.ai platform
  - [Tags: intermediate ibm multimodal-generation watsonx hands-on 2025]

- **[Hugging Face Transformers Course](https://huggingface.co/course)** - Official comprehensive course on using the Hugging Face Transformers library covering transformer models, fine-tuning techniques, training from scratch, and building multimodal NLP applications. Interactive course with hands-on exercises using PyTorch and TensorFlow.
  - ğŸ“– **Access:** 100% free, fully interactive  
  - ğŸ› ï¸ **Hands-on:** Yes (coding exercises throughout)  
  - ğŸ›ï¸ **Source:** Hugging Face (Official)  
  - **Topics:** Transformers architecture, Fine-tuning, Training from scratch, Multimodal NLP applications, Model deployment  
  - [Tags: `intermediate` `huggingface` `transformers` `nlp` `multimodal` `fine-tuning` `pytorch` `tensorflow` `2025`]

- **[A Survey of State of the Art Large Vision Language Models 2025 - arXiv](https://arxiv.org/pdf/2501.02189.pdf)** - Comprehensive survey of vision language models up to 2025 covering model architectures, alignment methods, popular benchmarks, evaluation metrics, and current challenges. Systematic overview of 20+ VLM developments including GPT-5V, Gemini-2, Claude-4V, and emerging models with technical comparisons.
  - ğŸ“– **Access:** Fully free (arXiv PDF)  
  - ğŸ›ï¸ **Authority:** arXiv (peer-reviewed research platform)  
  - ğŸ¯ **Level:** ğŸŸ¡ğŸ”´ Intermediate-Advanced  
  - **Focus:** Comprehensive VLM survey, architecture transitions, alignment methods, benchmarks  
  - **Scope:** Model overview, architecture details, evaluation frameworks, challenges and future directions  
  - **Models Covered:** GPT-5V, Gemini-2, Claude-4V, Llama-4Vision, Pixtral, DeepSeek-VL2, and 15+ others  
  - [Tags: `intermediate-advanced` `survey` `vlm-architectures` `2025-models` `benchmarks` `arxiv` `peer-reviewed` `2025`]

### ğŸ”´ Advanced

- **[Visionâ€“Language Models (VLMs): The Future of Multimodal AI - Code-B Dev](https://code-b.dev/blog/vision-llm)** - Comprehensive analysis of top 10 vision-language models in 2025 including GPT-5, Gemini 2, Claude 4, Gemma 3, and DeepSeek VL. Deep dive into VLM architectures (vision encoders, language models, fusion mechanisms), capabilities across modalities, and emerging challenges including bias, transparency, and responsible deployment.
  - ğŸ“– **Access:** Fully free  
  - ğŸ¯ **Level:** ğŸŸ¡ğŸ”´ Intermediate-Advanced  
  - **Top 10 Models:** GPT-5, Gemini 2, Claude 4, Gemma 3, DeepSeek VL, Qwen, LLaVA, Pixtral, others  
  - **Focus:** Architecture, capabilities, reasoning, challenges, future directions  
  - [Tags: `advanced` `vision-language-models` `top-10` `2025` `vlm-architecture` `challenges` `code-b`]

- **[Vision Language Models (Better, faster, stronger) - Hugging Face Blog](https://huggingface.co/blog/vlms-2025)** - Latest developments in vision language models for 2025 covering improvements in speed, accuracy, and capabilities. Deep analysis of Mixture-of-Experts (MoE) decoders for enhanced performance (Kimi-VL reasoning, MoE-LLaVA efficiency, DeepSeek-VL2), multimodal retrieval systems for PDF processing, and Llama 4 vision capabilities.
  - ğŸ“– **Access:** Fully free  
  - ğŸ›ï¸ **Authority:** Hugging Face (Official Blog)  
  - ğŸ¯ **Level:** ğŸ”´ Advanced  
  - **Focus:** 2025 improvements, MoE architectures, multimodal retrieval, latest models  
  - **Key Models:** Kimi-VL, MoE-LLaVA, DeepSeek-VL2, Llama 4 Vision  
  - **Topics:** Mixture-of-Experts decoders, Multimodal retrievers, Performance benchmarks, Emerging architectures  
  - [Tags: `advanced` `2025-developments` `moe-architectures` `multimodal-retrieval` `huggingface-blog` `llama-4` `kimi-vl` `2025`]

- **[Best Open-Source Vision Language Models of 2025 - LabellerR](https://www.labellerr.com/blog/top-open-source-vision-language-models/)** - In-depth technical comparison of leading open-source vision-language models in 2025. Detailed breakdown of Qwen 2.5 VL 72B Instruct, LLaMA 3.2 Vision (multi-size options), DeepSeek-VL with Mixture-of-Experts, Phi 4 Multimodal, and other cutting-edge models with technical specifications, use cases, and performance characteristics.
  - ğŸ“– **Access:** Fully free  
  - ğŸ¯ **Level:** ğŸ”´ Advanced  
  - **Focus:** Technical specifications, use cases, comparative analysis  
  - **Models:** Qwen 2.5 VL, LLaMA 3.2-Vision, DeepSeek-VL, Phi 4 Multimodal, others  
  - **Technical Details:** Parameters, architecture, training data, strengths, license  
  - [Tags: `advanced` `technical-comparison` `2025-models` `qwen-2.5-vl` `llama-3.2-vision` `deepseek-vl` `open-source` `labellerr` `2025`]

- **[Towards Comprehensive Benchmarking of Medical Vision Language Models - Oxford Academic](https://academic.oup.com/bib/article/26/Supplement_1/i24/8378044)** - Comprehensive benchmarking framework for evaluating medical vision-language models (VLMs). Compares 24 state-of-the-art MLLMs for medical imaging including CheXzero, MedCLIP, XrayGPT, LLaVA-Med, MedFILIP, and MedBridge. Covers evaluation metrics (ROUGE, F1, AUROC), efficiency measures (VRAM, latency), trust dimensions (factuality, bias, robustness), and parameter-efficient fine-tuning analysis on medical imaging datasets.
  - ğŸ“– **Access:** Fully free (Oxford Academic)  
  - ğŸ›ï¸ **Authority:** Oxford University Press / Peer-reviewed Journal  
  - ğŸ¯ **Level:** ğŸ”´ Advanced  
  - **Application:** Medical imaging, healthcare AI, VLM evaluation  
  - **Models Evaluated:** 24 VLMs including CheXzero, MedCLIP, XrayGPT, LLaVA-Med, MedBridge  
  - **Benchmarks:** IU-CXR, CT, MRI, ophthalmology datasets  
  - **Metrics:** ROUGE, F1-score, AUROC, VRAM, latency, factuality, bias, robustness  
  - [Tags: `advanced` `medical-ai` `benchmark` `vlm-evaluation` `healthcare-ai` `peer-reviewed` `2025`]

- **[A Benchmark Multimodal Oro-Dental Dataset for Large Vision-Language Models - arXiv](https://arxiv.org/abs/2511.04948)** - Advanced research dataset with 8,775 dental checkups from 4,800 patients (2018-2025) including 50,000 intraoral images, 8,056 radiographs, and detailed clinical notes. Demonstrates fine-tuning state-of-the-art VLMs (Qwen-VL 3B/7B) on medical tasks including 6-anomaly classification and diagnostic report generation. Validates effectiveness of domain-specific multimodal datasets for healthcare applications.
  - ğŸ“– **Access:** Fully free (arXiv publication)  
  - ğŸ›ï¸ **Authority:** arXiv research dataset  
  - ğŸ¯ **Level:** ğŸ”´ Advanced  
  - **Application:** Healthcare AI, dental diagnostics, medical multimodal learning  
  - **Dataset Size:** 8,775 checkups, 50K intraoral images, 8K radiographs, textual clinical notes  
  - **Models:** Qwen-VL 3B/7B fine-tuning, GPT-4o comparison  
  - **Tasks:** Anomaly classification (6 conditions), diagnostic report generation  
  - [Tags: `advanced` `medical-multimodal` `dental-dataset` `qwen-vl` `healthcare-ai` `benchmark` `2025`]

- **[Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey - arXiv](https://arxiv.org/abs/2508.13073)** - Comprehensive survey of Vision-Language-Action (VLA) models for robotics. Systematic review of large VLM-based architectures for robotic manipulation covering monolithic and hierarchical designs, integration with reinforcement learning, world models, and emerging capabilities like memory mechanisms and multi-agent cooperation. State-of-the-art in embodied AI and visual reasoning for robot control.
  - ğŸ“– **Access:** Fully free (arXiv PDF)  
  - ğŸ›ï¸ **Authority:** arXiv research survey  
  - ğŸ¯ **Level:** ğŸ”´ Advanced  
  - **Application:** Robotics, embodied AI, vision-language-action understanding  
  - **Focus:** VLA model architectures, monolithic vs hierarchical designs, RL integration, world models  
  - **Emerging Areas:** Memory mechanisms, 4D perception, efficient adaptation, multi-agent cooperation  
  - **Key Topics:** Visual perception for robot control, goal specification, action generation, generalization  
  - [Tags: `advanced` `robotics` `vision-language-action` `embodied-ai` `survey` `arxiv` `2025`]

- **[MMRL: Multi-Modal Representation Learning for Vision-Language Models - arXiv](https://arxiv.org/abs/2503.08497)** - Research on multi-modal representation learning (MMRL) addressing few-shot adaptation and overfitting challenges in VLMs. Novel approach to improving transfer learning performance with limited data by enhancing cross-modal alignment and feature representation. Addresses practical challenges in deploying VLMs to new domains with limited training examples.
  - ğŸ“– **Access:** Fully free (arXiv PDF)  
  - ğŸ›ï¸ **Authority:** arXiv (peer-reviewed research)  
  - ğŸ¯ **Level:** ğŸ”´ Advanced  
  - **Research Focus:** Representation learning, few-shot adaptation, overfitting prevention  
  - **Problem:** Limited data adaptation for VLMs, generalization to new tasks  
  - **Solution:** Enhanced cross-modal alignment, feature representation learning  
  - **Applications:** Domain adaptation, transfer learning, few-shot scenarios  
  - [Tags: `advanced` `multimodal-representation-learning` `few-shot-learning` `domain-adaptation` `arxiv` `research` `2025`]

- **[Magma: A Foundation Model for Multimodal AI Agents - arXiv](https://arxiv.org/abs/2502.13130)** - Advanced foundation model for multimodal AI agents combining vision-language understanding with spatial-temporal reasoning for agentic tasks. Extends VLM capabilities to include planning and action in digital and physical worlds, enabling UI navigation and robot manipulation with unified multimodal perception and reasoning.
  - ğŸ“– **Access:** Fully free (arXiv PDF)  
  - ğŸ›ï¸ **Authority:** arXiv (peer-reviewed research)  
  - ğŸ¯ **Level:** ğŸ”´ Advanced  
  - **Application:** Multimodal agents, robotics, digital task automation, embodied AI  
  - **Capabilities:** Verbal intelligence (VL understanding), spatial-temporal intelligence (planning, action)  
  - **Tasks:** UI navigation, robot manipulation, agentic reasoning across modalities  
  - **Innovation:** Unified perception-action framework, spatial reasoning in visual world  
  - [Tags: `advanced` `multimodal-agents` `foundation-models` `embodied-ai` `robotics` `arxiv` `2025`]

---

## ğŸ› ï¸ Key Models & Frameworks

**Popular Multimodal Models:**
- **CLIP** (OpenAI) - Vision-language contrastive learning
- **DALLÂ·3** (OpenAI) - Text-to-image generation
- **GPT-4V** (OpenAI) - Multimodal large language model
- **Flamingo** (DeepMind) - Few-shot visual question answering
- **Whisper** (OpenAI) - Speech recognition and translation
- **LLaVA** - Large Language and Vision Assistant
- **BLIP-2** - Bootstrapping vision-language pretraining
- **Qwen-VL** - Versatile vision-language model
- **Gemma 3** - Multimodal capabilities
- **DeepSeek-VL2** - Advanced vision-language reasoning

**Frameworks & Tools:**
- **Hugging Face Transformers** - Multimodal model implementations
- **LangChain** - Multimodal agent workflows
- **LlamaIndex** - Multimodal data indexing
- **OpenCLIP** - Open source CLIP implementations

---

## ğŸ”— Related Resources

**See also:**
- [Generative AI](./generative-ai.md) - Generative models and LLM techniques
- [Computer Vision](./computer-vision.md) - Vision-specific deep learning
- [Natural Language Processing](./natural-language-processing.md) - Text understanding and generation
- [Prompt Engineering](./prompt-engineering.md) - Crafting multimodal prompts
- [Audio & Speech Processing](./audio-speech-processing.md) - Audio modality
- [Robotics & Embodied AI](./robotics-embodied-ai.md) - Vision-language-action models
- [AI for Healthcare](./ai-for-healthcare.md) - Medical multimodal AI applications

**Cross-reference:**
- [Deep Learning & Neural Networks](./deep-learning-neural-networks.md) - Transformer architectures
- Open Source Models with Hugging Face also listed in [Generative AI](./generative-ai.md)
- Vision-Language Models Guide in [Generative AI](./generative-ai.md) Documentation section

---

## ğŸ¤ Contributing

Found a great free Multimodal AI resource? We'd love to add it!

**To contribute, use this format:**
```
- [Resource Name](URL) â€“ Clear description highlighting value and what you'll learn. (Difficulty Level)
  - ğŸ“– Access: [access details]
  - [Tags: keyword1 keyword2 keyword3]
```

**Ensure all resources are:**
- âœ… Completely free to access (no payment required)
- âœ… Openly available (no authentication barriers for core content)
- âœ… High-quality and educational
- âœ… Relevant to Multimodal AI
- âœ… From reputable sources (official docs, universities, established platforms)

---

**Last Updated:** January 8, 2026 | **Total Resources:** 24 (+7 new)

**Keywords:** multimodal-ai, vision-language-models, cross-modal-learning, multimodal-generation, clip, dall-e, flamingo, gpt-4v, multimodal-transformers, audio-visual-learning, multimodal-llms, text-to-image, image-captioning, vqa, cross-modal-retrieval, huggingface, transformers, deeplearning-ai, llava, vision-language-action, robotics